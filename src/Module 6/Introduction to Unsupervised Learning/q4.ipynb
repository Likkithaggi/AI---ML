{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns cannot be a set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4332/1299380402.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mone_hot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# One-hot encode the transaction data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mone_hot_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_transactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Calculate support for itemsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4332/1299380402.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Create one-hot encoded dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# GH47215\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns cannot be a set"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Sample transaction data\n",
    "dataset = [\n",
    "    ['Milk', 'Bread', 'Butter'],\n",
    "    ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
    "    ['Milk', 'Bread', 'Butter', 'Diapers'],\n",
    "    ['Bread', 'Milk', 'Butter', 'Diapers', 'Beer'],\n",
    "    ['Milk', 'Bread', 'Diapers', 'Beer'],\n",
    "    ['Bread', 'Milk', 'Diapers', 'Butter', 'Beer'],\n",
    "    ['Milk', 'Bread', 'Butter', 'Diapers'],\n",
    "]\n",
    "\n",
    "# Convert the dataset into a DataFrame for convenience\n",
    "df = pd.DataFrame(dataset, columns=['Item 1', 'Item 2', 'Item 3', 'Item 4', 'Item 5'])\n",
    "\n",
    "# Function to one-hot encode the dataset\n",
    "def encode_transactions(df):\n",
    "    items = set()\n",
    "    for transaction in df.values.flatten():\n",
    "        if isinstance(transaction, str):\n",
    "            items.add(transaction)\n",
    "    \n",
    "    # Create one-hot encoded dataframe\n",
    "    one_hot = pd.DataFrame(columns=items, index=df.index).fillna(0)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        for item in row:\n",
    "            if pd.notna(item):\n",
    "                one_hot.at[idx, item] = 1\n",
    "    return one_hot\n",
    "\n",
    "# One-hot encode the transaction data\n",
    "one_hot_df = encode_transactions(df)\n",
    "\n",
    "# Calculate support for itemsets\n",
    "def get_support(itemset, transactions):\n",
    "    itemset_len = len(itemset)\n",
    "    count = 0\n",
    "    for transaction in transactions:\n",
    "        if all(item in transaction for item in itemset):\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "# Find frequent itemsets using the Apriori algorithm\n",
    "def apriori(transactions, min_support):\n",
    "    itemsets = []\n",
    "    unique_items = set([item for transaction in transactions for item in transaction])\n",
    "    \n",
    "    # Generate frequent itemsets with 1 item\n",
    "    single_itemsets = [[item] for item in unique_items]\n",
    "    itemsets.extend(single_itemsets)\n",
    "    \n",
    "    # Filter itemsets based on support\n",
    "    frequent_itemsets = []\n",
    "    for itemset in itemsets:\n",
    "        support = get_support(itemset, transactions)\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets.append((itemset, support))\n",
    "    \n",
    "    # Generate itemsets with more than 1 item\n",
    "    k = 2\n",
    "    while True:\n",
    "        candidate_itemsets = []\n",
    "        for itemset1 in frequent_itemsets:\n",
    "            for itemset2 in frequent_itemsets:\n",
    "                combined = list(set(itemset1[0] + itemset2[0]))\n",
    "                if len(combined) == k and combined not in candidate_itemsets:\n",
    "                    candidate_itemsets.append(combined)\n",
    "        \n",
    "        new_frequent_itemsets = []\n",
    "        for itemset in candidate_itemsets:\n",
    "            support = get_support(itemset, transactions)\n",
    "            if support >= min_support:\n",
    "                new_frequent_itemsets.append((itemset, support))\n",
    "        \n",
    "        if not new_frequent_itemsets:\n",
    "            break\n",
    "        \n",
    "        frequent_itemsets.extend(new_frequent_itemsets)\n",
    "        k += 1\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Function to generate association rules from frequent itemsets\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for itemset, support in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            subsets = [list(combinations(itemset, i)) for i in range(1, len(itemset))]\n",
    "            for subset_list in subsets:\n",
    "                for subset in subset_list:\n",
    "                    antecedent = list(subset)\n",
    "                    consequent = list(set(itemset) - set(antecedent))\n",
    "                    confidence = get_support(antecedent + consequent, transactions) / get_support(antecedent, transactions)\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence, support))\n",
    "    return rules\n",
    "\n",
    "# Run Apriori Algorithm on the dataset\n",
    "min_support = 0.4\n",
    "min_confidence = 0.6\n",
    "frequent_itemsets = apriori(dataset, min_support)\n",
    "\n",
    "# Generate Association Rules\n",
    "rules = generate_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "# Display the rules\n",
    "for rule in rules:\n",
    "    print(f\"Rule: {rule[0]} -> {rule[1]} | Confidence: {rule[2]:.2f} | Support: {rule[3]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
